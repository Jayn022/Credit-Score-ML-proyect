{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc643f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import janitor\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import smogn\n",
    "import optuna\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.metrics import  mean_absolute_error,accuracy_score, classification_report, confusion_matrix, roc_auc_score, mean_squared_error, r2_score, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "#from statsmodels.tools.tools import add_constant\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') #Se cargan todas las librerias necesarias para utilizar (Algunas no estan siendo utilizadas porque se utilizaron anteriormente como medida de prevension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  filepath = 'datos_sinteticos.csv'\n",
    "  df= pd.read_csv(filepath)\n",
    "  df= pd.DataFrame(df)\n",
    "  df_clean= df.clean_names()\n",
    "  print(f\"El archivo se ha cargado y limpieado correctamente\")\n",
    "except FileNotFoundError:\n",
    "   print(f\"El archivo no ha sido encontrado\")  #Cargamos el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Informacion general del dataset\n",
    "print(f'''\\nINFORMACIÓN GENERAL DEL DATASET:\\n\n",
    "    - Dimensiones: {df_clean.shape[0]} filas x {df_clean.shape[1]} columnas \\n\n",
    "    - Período de análisis: dataset historico de préstamos bancarios\n",
    "''')\n",
    "\n",
    "print(f'''\n",
    "    \\n📊 ESTRUCTURA DEL DATASET: \\n{df_clean.info} \\n\n",
    "    \\n📈 ESTADÍSTICA DESCRIPTIVA: \\n {df_clean.describe()}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f580c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Busco valores faltantes dentro del archivo\n",
    "print('Valores faltantes:')\n",
    "missing_values = df_clean.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_clean)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Faltantes': missing_values, \n",
    "    'Porcentaje': missing_percentage\n",
    "}).sort_values('Porcentaje',ascending=False)\n",
    "print(missing_df[missing_df['Valores Faltantes'] > 0])\n",
    "\n",
    "calificacion_distribution = df_clean['calificacion'].value_counts()\n",
    "calificacion_percentage = calificacion_distribution.value_counts(normalize=True)  * 100\n",
    "#Reviso los valores unicos y la distribucion de estos, es decir cuantos hay\n",
    "print(f'''\n",
    "    \\n📊 · Valores únicos en Calificacion: {df_clean['calificacion'].unique()}\\n\n",
    "    \\n📈 · Distribución): \\n{calificacion_distribution}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Otorgo un significado a las variables de la columna que quiero predecir\n",
    "df_clean['calificacion_label'] = df_clean['calificacion'].map({''\n",
    "  'A1':'Aprobado',\n",
    "  'A2':'Aprobado',\n",
    "  'B1':'Aprobado',\n",
    "  'C1':'Aprobado con seguimiento',\n",
    "  'D1': 'No Aprobado',\n",
    "  'E1': 'No Aprobado'})\n",
    "label_distribution = df_clean['calificacion_label'].value_counts()\n",
    "label_percentage = label_distribution.value_counts(normalize=True) * 100\n",
    "\n",
    "calificacion_distribution = df_clean['calificacion'].value_counts().sort_index()\n",
    "calificacion_percentage = df_clean['calificacion'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"\\n📈 DISTRIBUCIÓN NUMÉRICA:\")\n",
    "for valor in sorted(calificacion_distribution.index):\n",
    "    print(f\"  {valor}: {calificacion_distribution[valor]:,} casos ({calificacion_percentage[valor]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_distribution = df_clean['calificacion_label'].value_counts()\n",
    "fig, ax = plt.subplots(2,2, figsize=(15,12))\n",
    "fig.suptitle('# de calificaciones en archivo', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Gráfico 1: Cliente vs puntaje final\n",
    "sns.scatterplot(data=df_clean, x='edad_cliente', y='puntaje', hue='calificacion', alpha=0.6, ax=ax[0,0])\n",
    "ax[0,0].set_title('Puntaje  vs Edad del Cliente', fontsize=14, fontweight='bold')\n",
    "ax[0,0].set_xlabel('Edad del Cliente', fontsize=12)\n",
    "ax[0,0].set_ylabel('Puntaje Final', fontsize=12)\n",
    "ax[0,0].legend(title='Calificación')\n",
    "\n",
    "\n",
    "# Gráfico 2: Distribución por edad\n",
    "ax[0,1].hist(df_clean['edad_cliente'], bins=20, alpha=.7, color='#66b3ff', edgecolor='black')\n",
    "ax[0,1].set_title('Distribución por Edad', fontsize=14, fontweight='bold')\n",
    "ax[0,1].set_xlabel('Edad', fontsize=12)\n",
    "ax[0,1].set_ylabel('Frecuencia', fontsize=12)\n",
    "\n",
    "# Gráfico 3: Ingresos VS Calificacion\n",
    "sns.boxplot(data=df_clean, x='calificacion', y='ingresos', palette='Set2', ax=ax[1,0])\n",
    "ax[1,0].set_yscale('log')\n",
    "ax[1,0].set_title('Ingresos por Estado de Calificación', fontsize=14, fontweight='bold')\n",
    "ax[1,0].set_xlabel('Calificación', fontsize=12)\n",
    "ax[1,0].set_ylabel('Ingresos (escala logarítmica)', fontsize=12)\n",
    "\n",
    "\n",
    "# Gráfico 4: Relación Deuda-Ingreso VS Default\n",
    "sns.boxplot(x='mora', y='ingresos', data=df_clean, ax=ax[1, 1], palette=[\"#de6767\", \"#1bda91\"])\n",
    "ax[1,1].set_title('Relación Mora e Ingreso', fontsize=14, fontweight='bold')\n",
    "ax[1,1].set_xlabel('Relación Mora', fontsize=12)\n",
    "ax[1,1].set_ylabel('Estado de ingreso', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1eedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comienzo con la separacion de los datos que quiero utilizar (x) para predecir la columna objetivo\n",
    "y= df_clean['calificacion']\n",
    "x= df_clean[['saldo','ahorro_general','sexo','saldo_por_vencer','saldo_pendiente','provision','dias_mora','edad_cliente','estado_civil','mora','antiguedad_meses','ingresos','descuentos','puntaje']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5aa977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba sacando valores no relevantes segun maquina\n",
    "df_test = df_clean[df_clean['puntaje'] <= 500].copy()\n",
    "Cols_predict = df_test[['saldo','ahorro_general','sexo','saldo_por_vencer','saldo_pendiente','provision','dias_mora','edad_cliente','estado_civil','mora','antiguedad_meses','ingresos','descuentos','puntaje']]\n",
    "df_balanced = smogn.smoter(data=Cols_predict, y='puntaje')\n",
    "x_balanced=df_balanced.drop('puntaje', axis=1)\n",
    "y_balanced=df_balanced['puntaje']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= pd.get_dummies(x, drop_first=True) #Asigna valores binarios para los datos con palabras\n",
    "x_balanced=pd.get_dummies(x_balanced, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(x_balanced)\n",
    "\n",
    "kmeans = KMeans(n_clusters=9, random_state=42)\n",
    "x_balanced['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) #Dividimos los datos 80% entrenamiento 20% prueba\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_balanced, y_balanced, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero un pipeline (defino una cadena de eventos para el momento que lo llame) con el que pueda entrenar, modelar, equilibrar, ejecutar y medir su score para saber que tan acertiva es.\n",
    "def pipeline(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\",50,70)\n",
    "    max_depth= trial.suggest_int(\"max_depth\",25,32)\n",
    "    min_samples_split= trial.suggest_int(\"min_samples_split\",15,25)\n",
    "    min_samples_leaf= trial.suggest_int(\"min_samples_leaf\",4,8)\n",
    "    \n",
    "    pipeline = imbpipeline([\n",
    "            ('Sm',SMOTE(random_state=42)),\n",
    "            ('RFC',RandomForestClassifier( \n",
    "                n_estimators= n_estimators,         \n",
    "                max_depth=max_depth,            \n",
    "                min_samples_split=min_samples_split,     \n",
    "                min_samples_leaf=min_samples_leaf,       \n",
    "                max_features=0.2,      \n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'\n",
    "            ))\n",
    "        ])\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(pipeline, x_train, y_train, cv=skf, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def pipeline_lineal(trial):\n",
    "#    n_estimators = trial.suggest_int(\"n_estimators\",80,160)\n",
    "#    max_depth= trial.suggest_int(\"max_depth\",60,80)\n",
    "#    min_samples_split= trial.suggest_int(\"min_samples_split\",25,40)\n",
    "#    min_samples_leaf= trial.suggest_int(\"min_samples_leaf\",10,20)\n",
    "#    \n",
    "#    pipeline2 = imbpipeline([\n",
    "#                ('RFC',RandomForestRegressor(\n",
    "#                n_estimators = n_estimators,\n",
    "#                max_depth=max_depth,            \n",
    "#                min_samples_split=min_samples_split,     \n",
    "#                min_samples_leaf=min_samples_leaf,       \n",
    "#                max_features=0.2,      \n",
    "#                random_state=42,\n",
    "#                n_jobs=-1,\n",
    "#            ))\n",
    "#        ])\n",
    "#    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#    scores2 = cross_val_score(pipeline2, x_train2, y_train2, cv=kf, scoring='r2')\n",
    "#    return scores2.mean()\n",
    "\n",
    "def objetivo(trial):\n",
    "   n_estimators= trial.suggest_int('n_estimators',100,400)\n",
    "   learning_rate= trial.suggest_float('learning_rate',0.01,0.4, log=True)\n",
    "   max_depth=trial.suggest_int('max_depth',3,20)\n",
    "   subsample= trial.suggest_float('subsample',0.5,1.0)\n",
    "   colsample_bytree= trial.suggest_float('colsample_bytree',0.3,1.0)\n",
    "   gamma=trial.suggest_float('gamma',0,5.0)\n",
    "   reg_alpha=trial.suggest_float('reg_alpha',0.0,10.0)\n",
    "   reg_lambda= trial.suggest_float('reg_lambda',0.0,10.0)\n",
    "        \n",
    "   modelo= xgb.XGBRegressor(\n",
    "      n_estimators=n_estimators,\n",
    "      max_depth=max_depth,\n",
    "      learning_rate=learning_rate,\n",
    "      subsample=subsample,\n",
    "      colsample_bytree=colsample_bytree,\n",
    "      gamma=gamma,\n",
    "      reg_alpha=reg_alpha,\n",
    "      reg_lambda=reg_lambda,\n",
    "      random_state=42,\n",
    "      n_jobs=-1\n",
    "   )\n",
    "   kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "   score = cross_val_score(modelo,x_train2,y_train2, cv=kf, scoring='r2')\n",
    "   \n",
    "   return score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizo optuna para que me ayude a sacar la mejor version posible del pipeline con 150 intentos.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(pipeline, n_trials=5)\n",
    "\n",
    "study2 = optuna.create_study(direction=\"maximize\")\n",
    "study2.optimize(objetivo, n_trials=300) \n",
    "\n",
    "#study3 = optuna.create_study(direction=\"maximize\")\n",
    "#study3.optimize(pipeline_lineal, n_trials=60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a213c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encapsulo los mejores parametros y los utilizo para el modelo de machine learning\n",
    "best_parameters=study.best_params\n",
    "best_parameters2=study2.best_params\n",
    "#best_parameters3=study3.best_params\n",
    "\n",
    "pipeline = imbpipeline([\n",
    "    ('Sm',SMOTE(random_state=42)),\n",
    "    ('RFC',RandomForestClassifier( \n",
    "        n_estimators= best_parameters['n_estimators'],         \n",
    "        max_depth=best_parameters['max_depth'],            \n",
    "        min_samples_split=best_parameters['min_samples_split'],     \n",
    "        min_samples_leaf=best_parameters['min_samples_leaf'],       \n",
    "        max_features=0.2,      \n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "      ))\n",
    "])\n",
    "\n",
    "modelo = xgb.XGBRegressor( \n",
    "        n_estimators= best_parameters2['n_estimators'],         \n",
    "        max_depth=best_parameters2['max_depth'],     \n",
    "        learning_rate=best_parameters2['learning_rate'],     \n",
    "        subsample=best_parameters2['subsample'],\n",
    "        colsample_bytree=best_parameters2['colsample_bytree'],\n",
    "        gamma=best_parameters2['gamma'],\n",
    "        reg_alpha=best_parameters2['reg_alpha'],\n",
    "        reg_lambda=best_parameters2['reg_lambda'],      \n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "      )\n",
    "\n",
    "def Linealpipeline_opt (trial):\n",
    "  k = trial.suggest_int('k', 3, x_train2.shape[1])\n",
    "\n",
    "  pipeline_xgb = imbpipeline([('scaler',StandardScaler()),\n",
    "    ('select',SelectKBest(f_regression, k=k)),\n",
    "    ('model',modelo)])\n",
    "  \n",
    "  pipeline_xgb.fit(x_train2,y_train2)\n",
    "\n",
    "  y_pred2 = pipeline_xgb.predict(x_test2)\n",
    "  r2 = r2_score(y_test2,y_pred2)\n",
    "  return r2\n",
    "\n",
    "  \n",
    "#pipeline2 = imbpipeline([\n",
    "#                ('RFC',RandomForestRegressor(\n",
    "#                n_estimators = best_parameters3['n_estimators'],\n",
    "#                max_depth=best_parameters3['max_depth'],        \n",
    "#                min_samples_split=best_parameters3['min_samples_split'],  \n",
    "#                min_samples_leaf=best_parameters3['min_samples_leaf'],       \n",
    "#                max_features=0.2,\n",
    "#                random_state=42,\n",
    "#                n_jobs=-1,\n",
    "#            ))\n",
    "#        ])\n",
    "\n",
    "#score_lineal= cross_val_score(pipeline2,x_train2,y_train2,scoring='r2', cv=10)\n",
    "#print(f\"Score de r2 sobre los datos {score_lineal.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b323f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "study3 = optuna.create_study(direction=\"maximize\")\n",
    "study3.optimize(Linealpipeline_opt, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lo entreno y comienzo a realizar las pruebas de que tan acertivo es\n",
    "pipeline.fit(x_train, y_train)\n",
    "modelo.fit(x_train2, y_train2)\n",
    "\n",
    "#Predicciones con los modelos\n",
    "y_predRFC = pipeline.predict(x_test)\n",
    "y_probaRFC = pipeline.predict_proba(x_test)[:, :]\n",
    "y_predRFC2 = modelo.predict(x_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = modelo.feature_importances_\n",
    "plt.barh(x_train2.columns, importances)\n",
    "plt.title(\"Importancia de características\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vuelvo a medir el modelo con los datos de entrenamiento, pero con el mejor pipeline para saber que tanto puede equivocarse\n",
    "scores= cross_val_score( pipeline, x_train,y_train, scoring='accuracy')\n",
    "scores2= cross_val_score( modelo, x_train2,y_train2, scoring='r2')\n",
    "#Matriz de Confusion\n",
    "#con esto mido que tanto se ha podido equivocar el modelo\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test,y_predRFC))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Matriz de Confusión\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69784f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy (RFC):\", accuracy_score(y_test, y_predRFC)) #Compara las precisiones\n",
    "\n",
    "print(\"Reporte de clasificación (RFC):\\n\", classification_report(y_test, y_predRFC)) #Puntuacion de datos por clase \n",
    "\n",
    "print(\"Matriz de confusión (RFC):\\n\", confusion_matrix(y_test, y_predRFC)) #Comparacion de aciertos por modelos\n",
    "\n",
    "print(\"ROC AUC (RFC):\", roc_auc_score(y_test, y_probaRFC, multi_class=\"ovr\")) # Hace una comparacion entre los verdaderos positivos y los falsos positivos (Roc: Receiver operating Characteristics; Mide el area bajo la curva, a mayor cercania de 1 mejor clasificacion (Auc: Area under the Curve))\n",
    "\n",
    "print(\"Los mejores parametros son: \", best_parameters)\n",
    "print(\"Accuracy en cada fold:\", scores)\n",
    "print(\"Accuracy promedio:\", np.mean(scores))\n",
    "print(\"Desviación estándar:\", np.std(scores))\n",
    "\n",
    "\n",
    "print(\"Maquina de regresion lineal\")\n",
    "\n",
    "mse = mean_squared_error(y_test2,y_predRFC2)\n",
    "mser = np.sqrt(mse)\n",
    "\n",
    "print(\"R2 Score:\", r2_score(y_test2, y_predRFC2))                # Qué tan bien el modelo explica la varianza\n",
    "print(\"MAE:\", mean_absolute_error(y_test2, y_predRFC2))          # Error promedio absoluto\n",
    "print(\"RMSE:\", mser)  # Raíz del error cuadrático medio\n",
    "\n",
    "\n",
    "print(\"Accuracy en cada fold:\", scores2)\n",
    "print(\"Accuracy promedio:\", np.mean(scores2))\n",
    "print(\"Desviación estándar:\", np.std(scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99169338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mido cuantos datos fueron a cada uno, es decir de todas las filas, cuales pertenecen a cada dato, y la relevancia de cada columna para el resultado.\n",
    "unique, counts = np.unique(y_train2, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "print(np.unique(y_train2, return_counts=True))\n",
    "\n",
    "importancia = modelo.feature_importances_\n",
    "columnas = x_train2.columns\n",
    "\n",
    "df_list = pd.DataFrame({'Variables' : columnas, 'Importancia': importancia})\n",
    "df_list = df_list.sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(df_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
